# -*- coding: utf-8 -*-
"""NextBestOffer_part1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1omyoAaviG7d0Z5UooRjcB-IQH7e1TYgj

# Библиотеки
"""

pip install catboost

from catboost import CatBoostClassifier

pip install scikit-plot

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scikitplot as skplt

from sklearn import metrics
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn. metrics import precision_recall_curve

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.isotonic import IsotonicRegression

from sklearn.calibration import calibration_curve
from sklearn.calibration import CalibratedClassifierCV

from sklearn.metrics import RocCurveDisplay

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/input data

"""# Модель 1: credit card - call

### I. Предобработка данных
"""

df1 = pd.read_csv("credit_card_call_train.csv")
df1_oot = pd.read_csv("credit_card_call_oot.csv")

df1['target'].value_counts()

"""Imbalanced data"""

def process_train_data(df: pd.DataFrame, df_oot: pd.DataFrame):
  x = df.drop(["target"], axis=1)
  y = df["target"]  #история: был отклик или нет

  x_oot = df_oot.drop(["target"], axis=1)
  y_oot = df_oot["target"]

  x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

  col_names = x_train.columns
  scaler = StandardScaler()

  x_train = scaler.fit_transform(x_train)
  x_test = scaler.fit_transform(x_val)
  x_oot = scaler.fit_transform(x_oot)

  x_train = pd.DataFrame(x_train,columns = col_names)
  x_test = pd.DataFrame(x_test,columns = col_names)
  x_oot = pd.DataFrame(x_oot,columns = col_names)

  selector = SelectKBest(k=50)
  selector.fit(x_train, y_train)

  vector_names = list(x_train.columns[selector.get_support(indices=True)])

  x_train = x_train[vector_names]
  x_val = x_val[vector_names]
  x_oot = x_oot[vector_names]

  return x_train, x_val, x_oot, y_train, y_val, y_oot

x1_train, x1_val, x1_oot, y1_train, y1_val, y1_oot = process_train_data(df1, df1_oot)

lr1 = LogisticRegression(random_state=42)
cbc1 = CatBoostClassifier(learning_rate=0.5, iterations=5)
RF1 = RandomForestClassifier()

lr1.fit(x1_train, y1_train)
cbc1.fit(x1_train, y1_train)
RF1.fit(x1_train, y1_train)

y1_train_cbc = cbc1.predict_proba(x1_train)[:,1]
y1_train_lr = lr1.predict_proba(x1_train)[:,1]
y1_train_rf = RF1.predict_proba(x1_train)[:, 1] # [0,1]

lr1_train = roc_auc_score(y1_train, y1_train_lr).round(3)
cbc1_train = roc_auc_score(y1_train, y1_train_cbc).round(3)
rf1_train = roc_auc_score(y1_train, y1_train_rf).round(3)

y1_val_cbc = cbc1.predict_proba(x1_val)[:,1]
y1_val_lr = lr1.predict_proba(x1_val)[:,1]
y1_val_rf = RF1.predict_proba(x1_val)[:, 1] # [0,1]

lr1_val = roc_auc_score(y1_val, y1_val_lr).round(3)
cbc1_val = roc_auc_score(y1_val, y1_val_cbc).round(3)
rf1_val = roc_auc_score(y1_val, y1_val_rf).round(3)

print("ROC-AUC lr_train: ",lr1_train)
print("ROC-AUC lr_val:   ",lr1_val)
print("ROC-AUC cbc_train:",cbc1_train)
print("ROC-AUC cbc_val:  ",cbc1_val)
print("ROC-AUC rf_train: ",rf1_train)
print("ROC-AUC rf_val:   ",rf1_val)

y1_oot_preds = cbc1.predict_proba(x1_oot)[:,1]

"""#### Графики"""

def ploting_roc_auc_curve(y_val: pd.DataFrame, y_val_model: pd.DataFrame, y_train: pd.DataFrame, y_train_model: pd.DataFrame):
  fig, ax = plt.subplots(figsize=(5,5))

  RocCurveDisplay.from_predictions(y_val, y_val_model, ax=ax, label='Test ROC')
  RocCurveDisplay.from_predictions(y_train, y_train_model, ax=ax, label='Train ROC')

  plt.title('ROC-AUC curve')
  plt.legend()
  plt.show()

ploting_roc_auc_curve(y1_val, y1_val_rf, y1_train, y1_train_rf)
ploting_roc_auc_curve(y1_val, y1_val_cbc, y1_train, y1_train_cbc)

"""**Note**: There is the trade-off between sensitivity (TPR) and specificity (FPR)

It shows TPR and FPR depending on the threshold we choose.
"""

def ploting_precision_recall(y_val: pd.DataFrame, y_val_model: pd.DataFrame, y_train: pd.DataFrame, y_train_model: pd.DataFrame):
  #calcultaing
  precision_t, recall_t, thresholds_t = precision_recall_curve(y_val, y_val_model)
  precision_v, recall_v, thresholds_v = precision_recall_curve(y_train, y_train_model)

  #create precision recall curve
  fig, ax = plt.subplots()
  ax.plot(recall_t, precision_t, color='purple')
  ax.plot(recall_v, precision_v, color='gray')

  #add axis labels to plot
  ax.set_title('Precision-Recall Curve')
  ax.set_ylabel('Precision')
  ax.set_xlabel('Recall')

  #display plot
  plt.show()

ploting_precision_recall(y1_val, y1_val_cbc, y1_train, y1_train_cbc)

"""**Note**: There is the trade-off between precision and recall

It shows TPR and FPR depending on the threshold we choose.

### II. Калибровка

Идеально откалиброванная модель - там, где факт и прогноз сошелся: сказали, что у 10 перцентиля сэмплов вероятность принадлежности к классу 1 равна 0.8 и выше, значит 80% из 10ого персентиля сэмплов должен иметь класс 1, а 20% - класс 0.

Калибровка строит еще одну модель над первой моделью
"""

def exp_cal_err(b, prb):
  """
  Возвращает ошибку калибровки
  """
  num_count, num_edges = np.histogram(prb, bins = 'fd') #вычисление гистограммы набора данных
  #'fd' — правило Фридмана-Диакониса
  nums = len(num_count) #количество ячеек
  num_edges[0] -= 1e-8 #левый край не включен
  num_id = np.digitize(prb, num_edges, right = True) - 1 #вычисление индексов числовых интервалов
  num_b_sum = np.bincount(num_id, weights = b, minlength = nums) #количество вхождений в массив
  num_prb_sum = np.bincount(num_id, weights = prb, minlength = nums)
  num_b_mean = np.divide(num_b_sum, num_count, out = np.zeros(nums), where = num_count > 0) #деление массивов
  num_prb_mean = np.divide(num_prb_sum, num_count, out = np.zeros(nums), where = num_count > 0)
  expcaliber = np.abs((num_prb_mean - num_b_mean) * num_count).sum() / len(prb) #конечная формула
  return expcaliber

#вариант 1 - calibrated classifier
cal_rf = CalibratedClassifierCV(cbc1, cv="prefit", method='sigmoid')
cal_rf.fit(x1_oot, y1_oot)
scores_sigm = cal_rf.predict_proba(x1_oot)[:, 1] #откалиброванный скор

#вариант 2 - isotonic calibrated classifier
cal_rf_iso = CalibratedClassifierCV(cbc1, cv="prefit", method='isotonic')
cal_rf_iso.fit(x1_oot, y1_oot)
scores1_iso = cal_rf_iso.predict_proba(x1_oot)[:, 1]  #откалиброванный скор

#Сравниваем ошибку калибровки
print("ECE на CV:             ", exp_cal_err(y1_oot, scores_sigm).round(4))
print("ECE на Isotonic CV:    ", exp_cal_err(y1_oot, scores1_iso).round(4))

"""#### Диаграммы - Распределение вероятностей по прогнозу и факту на oot"""

def ploting_diagram(prb: pd.DataFrame, y_oot: pd.DataFrame):
  frame = pd.DataFrame(columns=['score', 'target'])

  N_BUCCKET = 10

  frame['score'] = prb
  frame['target'] = y_oot
  frame['bucket'] = pd.qcut(frame.score.rank(method='first'), q=N_BUCCKET, labels=list(range(1,N_BUCCKET+1)),)

  gr_frame = frame.groupby('bucket').agg({
    'score': 'mean',
    'target': 'mean'
  }).reset_index()

  def plot_hist(x, y1, y2, label1, label2, name, bucket_cnt):
    xs = range(len(x))
    fig= plt.figure(figsize=(8,5))
    plt.bar([x+0.85  for x in xs], y1, width=0.3, color='lightsteelblue', alpha=0.8, label=label1, zorder=2)
    plt.bar([x+1.15 for x in xs], y2, width=0.3, color='darkblue', alpha=0.8, label=label2, zorder=2)
    plt.xticks([(x) for x in range(1, bucket_cnt+1)], size=18)
    plt.yticks(size=18)
    plt.legend(loc='upper left', prop={"size":12})
    plt.ylabel('Вероятность, %', size=12)
    plt.xlabel('Номер бакета по скору модели', size=12)
    plt.title(name, size=10)
    plt.savefig(f'parity.png', bbox_inches='tight')

  plot_hist(gr_frame.bucket,
          gr_frame.score,
          gr_frame.target,
          'Средняя прогнозная вероятность',
          'Средний фактический отклик',
          'Распределение прогнозной вероятности и фактического отклика \nна отложенной выборке',
          10)

ploting_diagram(y1_oot_preds, y1_oot)
ploting_diagram(scores1_iso, y1_oot)

"""#### Графики - Кривая калибровки"""

def ploting_calibration_curve(prb: pd.DataFrame, y_oot: pd.DataFrame, scores_iso: pd.DataFrame):
  #calculate
  prob_true, prob_pred = calibration_curve(y_oot, y_oot, n_bins=10)
  prob_true1, prob_pred1 = calibration_curve(y_oot, prb, n_bins=10)
  prob_true2, prob_pred2 = calibration_curve(y_oot, scores_iso, n_bins=10)

  #create curve
  fig, ax = plt.subplots()
  fig.suptitle('Calibration curve', fontsize=14, fontweight='bold')
  ax.plot(prob_true, prob_pred, color='purple')
  ax.plot(prob_true1, prob_pred1, color='blue')
  ax.plot(prob_true2, prob_pred2, color='gray')

ploting_calibration_curve(y1_oot_preds, y1_oot, scores1_iso)

"""# Модель 2: credit card - sms"""

df2 = pd.read_csv("credit_card_sms_train.csv")
df2_oot = pd.read_csv("credit_card_sms_oot.csv")

x2_train, x2_val, x2_oot, y2_train, y2_val, y2_oot = process_train_data(df2, df2_oot)

lr2 = LogisticRegression(random_state=42)
cbc2 = CatBoostClassifier(learning_rate=0.5, iterations=5)
RF2 = RandomForestClassifier()

lr2.fit(x2_train, y2_train)
cbc2.fit(x2_train, y2_train)
RF2.fit(x2_train, y2_train)

y2_train_cbc = cbc2.predict_proba(x2_train)[:,1]
y2_train_lr = lr2.predict_proba(x2_train)[:,1]
y2_train_rf = RF2.predict_proba(x2_train)[:, 1] # [0,1]

lr2_train = roc_auc_score(y2_train, y2_train_lr).round(3)
cbc2_train = roc_auc_score(y2_train, y2_train_cbc).round(3)
rf2_train = roc_auc_score(y2_train, y2_train_rf).round(3)

y2_val_cbc = cbc2.predict_proba(x2_val)[:,1]
y2_val_lr = lr2.predict_proba(x2_val)[:,1]
y2_val_rf = RF2.predict_proba(x2_val)[:, 1] # [0,1]

lr2_val = roc_auc_score(y2_val, y2_val_lr).round(3)
cbc2_val = roc_auc_score(y2_val, y2_val_cbc).round(3)
rf2_val = roc_auc_score(y2_val, y2_val_rf).round(3)

print("ROC-AUC lr_train: ",lr2_train)
print("ROC-AUC lr_val:   ",lr2_val)
print("ROC-AUC cbc_train:",cbc2_train)
print("ROC-AUC cbc_val:  ",cbc2_val)
print("ROC-AUC rf_train: ",rf2_train)
print("ROC-AUC rf_val:   ",rf2_val)

y2_oot_preds = lr2.predict_proba(x2_oot)[:,1]

"""#### Графики ROC AUC и Precision Recall"""

ploting_roc_auc_curve(y2_val, y2_val_rf, y2_train, y2_train_rf)
ploting_roc_auc_curve(y2_val, y2_val_lr, y2_train, y2_train_lr)

ploting_precision_recall(y2_val, y2_val_lr, y2_train, y2_train_lr)

"""### II. Калибровка"""

#вариант 1 - calibrated classifier
cal_rf = CalibratedClassifierCV(lr2, cv="prefit", method='sigmoid')
cal_rf.fit(x2_oot, y2_oot)
scores_sigm = cal_rf.predict_proba(x2_oot)[:, 1] #откалиброванный скор

#вариант 2 - isotonic calibrated classifier
cal_rf_iso = CalibratedClassifierCV(lr2, cv="prefit", method='isotonic')
cal_rf_iso.fit(x2_oot, y2_oot)
scores2_iso = cal_rf_iso.predict_proba(x2_oot)[:, 1]  #откалиброванный скор

#Сравниваем ошибку калибровки
print("ECE на CV:             ", exp_cal_err(y2_oot, scores_sigm).round(4))
print("ECE на Isotonic CV:    ", exp_cal_err(y2_oot, scores2_iso).round(4))

"""#### Диаграмма"""

ploting_diagram(y2_oot_preds, y2_oot)
ploting_diagram(scores2_iso, y2_oot)

"""#### Графики - кривая калибровки"""

ploting_calibration_curve(y2_oot_preds, y2_oot, scores2_iso)

"""# Модель 3: credit - call"""

df3 = pd.read_csv("credit_call_train.csv")
df3_oot = pd.read_csv("credit_call_oot.csv")

x3_train, x3_val, x3_oot, y3_train, y3_val, y3_oot = process_train_data(df3, df3_oot)

lr3 = LogisticRegression(random_state=42)
cbc3 = CatBoostClassifier(learning_rate=0.5, iterations=5)
RF3 = RandomForestClassifier()

lr3.fit(x3_train, y3_train)
cbc3.fit(x3_train, y3_train)
RF3.fit(x3_train, y3_train)

y3_train_cbc = cbc3.predict_proba(x3_train)[:,1]
y3_train_lr = lr3.predict_proba(x3_train)[:,1]
y3_train_rf = RF3.predict_proba(x3_train)[:, 1] # [0,1]

lr3_train = roc_auc_score(y3_train, y3_train_lr).round(3)
cbc3_train = roc_auc_score(y3_train, y3_train_cbc).round(3)
rf3_train = roc_auc_score(y3_train, y3_train_rf).round(3)

y3_val_cbc = cbc3.predict_proba(x3_val)[:,1]
y3_val_lr = lr3.predict_proba(x3_val)[:,1]
y3_val_rf = RF3.predict_proba(x3_val)[:, 1] # [0,1]

lr3_val = roc_auc_score(y3_val, y3_val_lr).round(3)
cbc3_val = roc_auc_score(y3_val, y3_val_cbc).round(3)
rf3_val = roc_auc_score(y3_val, y3_val_rf).round(3)

print("ROC-AUC lr_train: ",lr3_train)
print("ROC-AUC lr_val:   ",lr3_val)
print("ROC-AUC cbc_train:",cbc3_train)
print("ROC-AUC cbc_val:  ",cbc3_val)
print("ROC-AUC rf_train: ",rf3_train)
print("ROC-AUC rf_val:   ",rf3_val)

y3_oot_preds = lr3.predict_proba(x3_oot)[:,1]

ploting_roc_auc_curve(y3_val, y3_val_rf, y3_train, y3_train_rf)
ploting_roc_auc_curve(y3_val, y3_val_lr, y3_train, y3_train_lr)

ploting_precision_recall(y3_val, y3_val_lr, y3_train, y3_train_lr)

#вариант 1 - calibrated classifier
cal_rf = CalibratedClassifierCV(lr3, cv="prefit", method='sigmoid')
cal_rf.fit(x3_oot, y3_oot)
scores_sigm = cal_rf.predict_proba(x3_oot)[:, 1] #откалиброванный скор

#вариант 2 - isotonic calibrated classifier
cal_rf_iso = CalibratedClassifierCV(lr3, cv="prefit", method='isotonic')
cal_rf_iso.fit(x3_oot, y3_oot)
scores3_iso = cal_rf_iso.predict_proba(x3_oot)[:, 1]  #откалиброванный скор

#Сравниваем ошибку калибровки
print("ECE на CV:             ", exp_cal_err(y3_oot, scores_sigm).round(4))
print("ECE на Isotonic CV:    ", exp_cal_err(y3_oot, scores3_iso).round(4))

ploting_diagram(y3_oot_preds, y3_oot)
ploting_diagram(scores3_iso, y3_oot)

ploting_calibration_curve(y3_oot_preds, y3_oot, scores3_iso)

"""# Модель 4: credit - sms"""

df4 = pd.read_csv("credit_sms_train.csv")
df4_oot = pd.read_csv("credit_sms_oot.csv")

x4_train, x4_val, x4_oot, y4_train, y4_val, y4_oot = process_train_data(df4, df4_oot)

lr4 = LogisticRegression(random_state=42)
cbc4 = CatBoostClassifier(learning_rate=0.5, iterations=5)
RF4 = RandomForestClassifier()

lr4.fit(x4_train, y4_train)
cbc4.fit(x4_train, y4_train)
RF4.fit(x4_train, y4_train)

y4_train_cbc = cbc4.predict_proba(x4_train)[:,1]
y4_train_lr = lr4.predict_proba(x4_train)[:,1]
y4_train_rf = RF4.predict_proba(x4_train)[:, 1] # [0,1]

lr4_train = roc_auc_score(y4_train, y4_train_lr).round(3)
cbc4_train = roc_auc_score(y4_train, y4_train_cbc).round(3)
rf4_train = roc_auc_score(y4_train, y4_train_rf).round(3)

y4_val_cbc = cbc4.predict_proba(x4_val)[:,1]
y4_val_lr = lr4.predict_proba(x4_val)[:,1]
y4_val_rf = RF4.predict_proba(x4_val)[:, 1] # [0,1]

lr4_val = roc_auc_score(y4_val, y4_val_lr).round(3)
cbc4_val = roc_auc_score(y4_val, y4_val_cbc).round(3)
rf4_val = roc_auc_score(y4_val, y4_val_rf).round(3)

print("ROC-AUC lr_train: ",lr4_train)
print("ROC-AUC lr_val:   ",lr4_val)
print("ROC-AUC cbc_train:",cbc4_train)
print("ROC-AUC cbc_val:  ",cbc4_val)
print("ROC-AUC rf_train: ",rf4_train)
print("ROC-AUC rf_val:   ",rf4_val)

y4_oot_preds = cbc4.predict_proba(x4_oot)[:,1]

#ploting_roc_auc_curve(y4_val, y4_val_rf, y4_train, y4_train_rf)
ploting_roc_auc_curve(y4_val, y4_val_lr, y4_train, y4_train_lr)

ploting_precision_recall(y4_val, y4_val_cbc, y4_train, y4_train_cbc)

#вариант 1 - calibrated classifier
cal_rf = CalibratedClassifierCV(cbc4, cv="prefit", method='sigmoid')
cal_rf.fit(x4_oot, y4_oot)
scores_sigm = cal_rf.predict_proba(x4_oot)[:, 1] #откалиброванный скор

#вариант 2 - isotonic calibrated classifier
cal_rf_iso = CalibratedClassifierCV(cbc4, cv="prefit", method='isotonic')
cal_rf_iso.fit(x4_oot, y4_oot)
scores4_iso = cal_rf_iso.predict_proba(x4_oot)[:, 1]  #откалиброванный скор

#Сравниваем ошибку калибровки
print("ECE на CV:             ", exp_cal_err(y4_oot, scores_sigm).round(4))
print("ECE на Isotonic CV:    ", exp_cal_err(y4_oot, scores4_iso).round(4))

ploting_diagram(y4_oot_preds, y4_oot)
ploting_diagram(scores4_iso, y4_oot)

ploting_calibration_curve(y4_oot_preds, y4_oot, scores4_iso)

"""https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/

https://www.geeksforgeeks.org/precision-recall-curve-ml/

# Оптимизация

### I. Объединение 4 моделей
"""

proba1 = pd.DataFrame(scores1_iso, columns=['proba'])
proba1 = proba1.reset_index()

group_1 = pd.DataFrame(columns= ['client','product','channel','proba'])
group_1['client'] = proba1['index']
group_1['product'] = 'credit card'
group_1['channel'] = 'call'
group_1['proba'] = proba1['proba']

proba2 = pd.DataFrame(scores2_iso, columns=['proba'])
proba2 = proba1.reset_index()

group_2 = pd.DataFrame(columns= ['client','product','channel','proba'])
group_2['client'] = proba2['index']
group_2['product'] = 'credit card'
group_2['channel'] = 'sms'
group_2['proba'] = proba2['proba']

assert group_1.shape == (20000, 4)

proba3 = pd.DataFrame(scores3_iso, columns=['proba'])
proba3 = proba3.reset_index()

group_3 = pd.DataFrame(columns= ['client','product','channel','proba'])
group_3['client'] = proba3['index']
group_3['product'] = 'credit'
group_3['channel'] = 'call'
group_3['proba'] = proba3['proba']

proba4 = pd.DataFrame(scores4_iso, columns=['proba'])
proba4 = proba4.reset_index()

group_4 = pd.DataFrame(columns= ['client','product','channel','proba'])
group_4['client'] = proba4['index']
group_4['product'] = 'credit'
group_4['channel'] = 'sms'
group_4['proba'] = proba4['proba']

lst = group_1, group_2, group_3, group_4
db = pd.concat(lst)
frame = db

frame = pd.DataFrame(frame)
frame.to_csv('frame_final.csv')

"""https://jamesmccaffrey.wordpress.com/2021/01/06/calculating-expected-calibration-error-for-binary-classification/ - описанный на пальцах расчет ошибки калибровки

---


https://habr.com/ru/articles/648753/

https://hal.science/hal-04295601/document
"""
# -*- coding: utf-8 -*-
"""causal_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VEkOZOYp8ur-00B-GiA_Po_VZyTc2Dwk

## Библиотеки
"""

pip install pingouin

pip install dowhy

import time
from itertools import combinations
from tqdm import tqdm

import numpy as np
import pandas as pd
import networkx as nx
import pingouin as pg
from dowhy import CausalModel
import matplotlib.pyplot as plt

np.random.seed(0)

"""## Подгрузка данных"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/input data

"""## Обработка данных

### Основное
"""

df = pd.read_csv('ci_data_internship.csv').drop(columns=['Unnamed: 0'])

#df[df['id'].isna()]

#df[df['id']==10111922.0]

#df['address'].astype(str)

#извлечем город из адреса
df[["street","city","index"]]= df["address"].str.split(", ", expand=True)

len(df['city'].unique())
cities = df['city'].unique()

df.shape

df.nunique(axis=0)

#df[df['id'] == 10111423.0].shape

df.isnull().sum()

#удалим строки, где id магазина неизвестны
df.dropna(subset = ['id'], inplace=True)
#df['id'] = df['id'].fillna(method='ffill', limit=1)

df.isnull().sum()

#df.isnull().sum()
df.shape

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

df = df.drop(columns=['street', 'address','index'])

#закодируем город
df['city']= label_encoder.fit_transform(df['city'])

#все остальное заменим медианной
df = df.fillna(df.median())

#рассмотрим связь M_sales и S_sales
df[['M_sales', 'S_sales']].corr(method= 'pearson').round(2)

#введем переменную-счетчик id для анализа временных рядов

#df[148630:148640]
#df[df['M_sales'] != 0]
#df[(df['M_sales'] != 0) & (df['M_price'] != 0)]
df.sort_values(by='id')
# Add a column that counts occurrences starting from 1
df['occurrence_number'] = df.groupby('id').cumcount() + 1

"""По итогам консультации уберем id и адрес из графа"""

df = df.drop(columns=['id', 'city'])

df = df.drop(columns=['occurrence_number'])

df.shape

df

"""Сформируем разные датасеты на основе полученного"""

import numpy as np
mu, sigma = 0, 0.1
# создаем белый шум с размерностью датасета (513121,16)
noise = np.random.normal(mu, sigma, [513121,16])

df_s1 = df + noise

# не имеет смысла без id
#df_s2 = df[df['occurrence_number'] <= 7]

#берем 20% данных из датасета
df_s2 = df.sample(frac = 0.2)

"""### Карты"""

pip install geopy

from geopy.geocoders import Nominatim

def get_coordinates(city):
    geolocator = Nominatim(user_agent="my_app")
    location = geolocator.geocode(city)
    return location.latitude, location.longitude

#city = "New York"
cities_loc = []
for c in cities:
  latitude, longitude = get_coordinates(c)
  item = [c,latitude,longitude]
  cities_loc.append(item)

dct=[]
for i in cities_loc:
  item = {'city': i[0], 'coodrs': [i[1], i[2]]}
  dct.append(item)

#cities_loc

import folium

m = folium.Map(zoom_start=19,control_scale=True)

for counter, city in enumerate(dct):
    circle_size = 0.01 #+ counter
    folium.CircleMarker(
        location=city['coodrs'],
        radius=circle_size,
        popup=city['city'],
        color="crimson",
        fill=True,
        fill_color="crimson",
    ).add_to(m)

m.save('map.html')
m

"""## Строим граф"""

def nudge(pos, x_shift, y_shift):
    return {n:(x + x_shift, y + y_shift) for n,(x,y) in pos.items()}

# визуализация
def draw_graph(G, pos):
    colors = ['red' if (y,x) in G.edges() else 'black' for (x,y) in G.edges()]

    pos_nodes = nudge(pos, 0.0, -0.5)

    fig, ax = plt.subplots(1, figsize=(8,8))
    ax.margins(0.2)
    nx.draw(G, ax=ax, pos=pos, with_labels=False, node_size=500, node_color='w', edgecolors='black', edge_color=colors)
    nx.draw_networkx_labels(G, ax=ax, pos=pos_nodes, font_size=12)

nodes = list(df.columns)

true_graph = nx.DiGraph()
true_graph.add_nodes_from(nodes)
true_graph.add_edges_from([('is_holiday', 'S_sales'),
                           ('day_type', 'S_sales'),
                           ('M_sales', 'S_sales'),
                           ('S_price', 'S_sales'),
                           ('M_promo_internet', 'customers_number'),
                           ('M_promo_banners', 'customers_number'),])

# укладка графа
pos = {'season':(6.7, 6),  'menu_type':(3.7, 6), 'is_holiday':(10.7, 6), 'day_type':(11.7, 5.),
       'customers_number':(12, 4), 'comp_activity':(11.7, 3), 'M_promo_exp':(0.7, 3.5), 'M_promo_internet':(9.7, 2), 'M_promo_banners':(0.7, 5.5),
       'S_price':(6, 4.5), 'M_price':(7.5, 3.5), 'average_purchase_items':(1.6,4.5), 'average_paycheck':(2.3, 2.7),
       'repeated_customers_number':(2.3,1.9), 'M_sales':(6.2,2.9), 'S_sales':(4.5, 3.5), }

draw_graph(true_graph, pos)

"""## Constrained based алгоритмы

### Алгоритм PC
"""

alpha = 0.05

from causallearn.search.ConstraintBased.PC import pc
from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge

graph_pc_raw = pc(df.values, alpha=alpha, indep_test='fisherz')

graph_pc_raw.to_nx_graph()

mapping = {node:i for node, i in enumerate(df.columns)}
mapping_r = {i:node for node, i in enumerate(df.columns)}

# добавим корректные названия вершин в граф
graph_pc = nx.relabel_nodes(graph_pc_raw.nx_graph, mapping)

draw_graph(graph_pc, pos)

"""В библиотеке causal-learn имеется собственный класс для работы с экспертными знаниями:"""

backgroung_knowledge = BackgroundKnowledge()

# достаем вершины из GeneralGraph
nodes = graph_pc_raw.G.get_nodes()

df.columns

"""### Ноды"""

# удаляем возможность построить ребро в одном направлении
node_1 = nodes[mapping_r['is_holiday']]
node_2 = nodes[mapping_r['day_type']]
node_3 = nodes[mapping_r['season']]
node_4 = nodes[mapping_r['comp_activity']]
node_5 = nodes[mapping_r['M_promo_exp']]
node_6 = nodes[mapping_r['M_promo_internet']]
node_7 = nodes[mapping_r['M_promo_banners']]
node_8 = nodes[mapping_r['S_price']]
node_9 = nodes[mapping_r['M_price']]
node_10 = nodes[mapping_r['menu_type']]
node_11= nodes[mapping_r['average_purchase_items']]
node_12 = nodes[mapping_r['average_paycheck']]
node_13 = nodes[mapping_r['repeated_customers_number']]
node_14 = nodes[mapping_r['customers_number']]
node_15 = nodes[mapping_r['M_sales']]
node_16 = nodes[mapping_r['S_sales']]

# добавляем запрет на ребро аргумент_1 -> аргумент_2
backgroung_knowledge.add_forbidden_by_node(node_2, node_1)
backgroung_knowledge.add_forbidden_by_node(node_3, node_1)
backgroung_knowledge.add_forbidden_by_node(node_4, node_1)
backgroung_knowledge.add_forbidden_by_node(node_5, node_1)
backgroung_knowledge.add_forbidden_by_node(node_6, node_1)
backgroung_knowledge.add_forbidden_by_node(node_7, node_1)
backgroung_knowledge.add_forbidden_by_node(node_8, node_1)
backgroung_knowledge.add_forbidden_by_node(node_9, node_1)
backgroung_knowledge.add_forbidden_by_node(node_10, node_1)
backgroung_knowledge.add_forbidden_by_node(node_11, node_1)
backgroung_knowledge.add_forbidden_by_node(node_12, node_1)
backgroung_knowledge.add_forbidden_by_node(node_13, node_1)
backgroung_knowledge.add_forbidden_by_node(node_14, node_1)
backgroung_knowledge.add_forbidden_by_node(node_15, node_1)
backgroung_knowledge.add_forbidden_by_node(node_16, node_1)

backgroung_knowledge.add_forbidden_by_node(node_1, node_2)
backgroung_knowledge.add_forbidden_by_node(node_3, node_2)
backgroung_knowledge.add_forbidden_by_node(node_4, node_2)
backgroung_knowledge.add_forbidden_by_node(node_5, node_2)
backgroung_knowledge.add_forbidden_by_node(node_6, node_2)
backgroung_knowledge.add_forbidden_by_node(node_7, node_2)
backgroung_knowledge.add_forbidden_by_node(node_8, node_2)
backgroung_knowledge.add_forbidden_by_node(node_9, node_2)
backgroung_knowledge.add_forbidden_by_node(node_10, node_2)
backgroung_knowledge.add_forbidden_by_node(node_11, node_2)
backgroung_knowledge.add_forbidden_by_node(node_12, node_2)
backgroung_knowledge.add_forbidden_by_node(node_13, node_2)
backgroung_knowledge.add_forbidden_by_node(node_14, node_2)
backgroung_knowledge.add_forbidden_by_node(node_15, node_2)
backgroung_knowledge.add_forbidden_by_node(node_16, node_2)

backgroung_knowledge.add_forbidden_by_node(node_1, node_3)
backgroung_knowledge.add_forbidden_by_node(node_2, node_3)
backgroung_knowledge.add_forbidden_by_node(node_4, node_3)
backgroung_knowledge.add_forbidden_by_node(node_5, node_3)
backgroung_knowledge.add_forbidden_by_node(node_6, node_3)
backgroung_knowledge.add_forbidden_by_node(node_7, node_3)
backgroung_knowledge.add_forbidden_by_node(node_8, node_3)
backgroung_knowledge.add_forbidden_by_node(node_9, node_3)
backgroung_knowledge.add_forbidden_by_node(node_10, node_3)
backgroung_knowledge.add_forbidden_by_node(node_11, node_3)
backgroung_knowledge.add_forbidden_by_node(node_12, node_3)
backgroung_knowledge.add_forbidden_by_node(node_13, node_3)
backgroung_knowledge.add_forbidden_by_node(node_14, node_3)
backgroung_knowledge.add_forbidden_by_node(node_15, node_3)
backgroung_knowledge.add_forbidden_by_node(node_16, node_3)

backgroung_knowledge.add_forbidden_by_node(node_1, node_10)
backgroung_knowledge.add_forbidden_by_node(node_2, node_10)
backgroung_knowledge.add_forbidden_by_node(node_4, node_10)
backgroung_knowledge.add_forbidden_by_node(node_5, node_10)
backgroung_knowledge.add_forbidden_by_node(node_6, node_10)
backgroung_knowledge.add_forbidden_by_node(node_7, node_10)
backgroung_knowledge.add_forbidden_by_node(node_8, node_10)
backgroung_knowledge.add_forbidden_by_node(node_9, node_10)
backgroung_knowledge.add_forbidden_by_node(node_3, node_10)
backgroung_knowledge.add_forbidden_by_node(node_11, node_10)
backgroung_knowledge.add_forbidden_by_node(node_12, node_10)
backgroung_knowledge.add_forbidden_by_node(node_13, node_10)
backgroung_knowledge.add_forbidden_by_node(node_14, node_10)
backgroung_knowledge.add_forbidden_by_node(node_15, node_10)
backgroung_knowledge.add_forbidden_by_node(node_16, node_10)

"""### Построим граф с background knowledge:"""

graph_pc_raw_bk = pc(df.values, alpha=alpha, indep_test='fisherz', background_knowledge=backgroung_knowledge)

graph_pc_raw_bk.to_nx_graph()

# добавим корректные названия вершин в граф
graph_pc_bk = nx.relabel_nodes(graph_pc_raw_bk.nx_graph, mapping)

draw_graph(graph_pc_bk, pos)

"""## Score based алгоритмы

### GES

Алгоритм GES (Greedy Equivalence Search) оценивает соответствие потенциального каузального графа данным с помощью скоринговой функции (например, BIC).

 Алгоритм имеет схожие с PC допущения и включает в себя три шага:

Инициализация пустого графа;
Последовательное добавление ребер вплоть до достижения локального максимума скоринговой функции (forward phase);
Последовательное удаление ребер вплоть до достижения локального максимума скоринговой функции (backward phase).
Полученные промежуточные графы во время шагов 2 и 3 сопоставляются с соответствующими марковскими классами эквивалентности.
"""

from causallearn.search.ScoreBased.GES import ges

graph_ges = ges(df, score_func='local_score_BIC')

import matplotlib.pyplot as plt

import io
from causallearn.utils.GraphUtils import GraphUtils
import matplotlib.image as mpimg

pyd = GraphUtils.to_pydot(graph_ges['G'], labels=df.columns)
pyd.write_png('graph_ges.png')

image_path = "graph_ges.png"
image = mpimg.imread(image_path)
plt.imshow(image)
plt.show()

graph_ges_s1 = ges(df_s1, score_func='local_score_BIC')
pyd = GraphUtils.to_pydot(graph_ges_s1['G'], labels=df_s1.columns)
pyd.write_png('graph_ges_s1.png')

image_path = "graph_ges_s1.png"
image = mpimg.imread(image_path)
plt.imshow(image)
plt.show()

graph_ges_s2 = ges(df_s2, score_func='local_score_BIC')
pyd = GraphUtils.to_pydot(graph_ges_s2['G'], labels=df_s2.columns)
pyd.write_png('graph_ges_s2.png')

image_path = "graph_ges_s2.png"
image = mpimg.imread(image_path)
plt.imshow(image)
plt.show()

"""Вывод: видим, что алгоритм GES в общих чертах устойчив в выявлении вазимосвязей на разных срезах датасета."""